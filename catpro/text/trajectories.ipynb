{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create lexicons\n",
    "2. Encode words from lexicons\n",
    "3. Dimensionality reduction. Supervised UMAP\n",
    "4. Plot in 2D and color code by lexicon\n",
    "5. Repeat 2,3,4 for participant's text, but plot one at a time. Save individual trajectory\n",
    "\n",
    "Different analyses:\n",
    "\n",
    "- A) Cluster longitudinal trajectories [X,Y]\n",
    "\n",
    "- B) Compute distance between trajectories and lexicon centroids or Hausdorff distance with all lexicon words to obtain 2D array (time points, lexicon categories). You can take median and IQR. \n",
    "\n",
    "- C) Semantic Tracker original idea: compute similarity with lexicon words and take median, upper 25%, etc. \n",
    "\n",
    "Then compare to rating scales using CCA\n",
    "- A) (Rating scales, A)\n",
    "- B) (Rating scales, B)\n",
    "- C) (Rating scales, C)\n",
    "\n",
    "\n",
    "- Then you can plot ratio (active vs. passive), etc. \n",
    "- Plot trajectories of subtypes \n",
    "\n",
    "- Compare approach to co-occurance approach. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from importlib import reload\n",
    "# sys.path.insert(1, '/../catpro/catpro')# until pip install is ready\n",
    "sys.path.append('./../catpro')\n",
    "from catpro.data.lexicons import stb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catpro import encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lexicons = stb.stb\n",
    "print(lexicons.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(input_dir+f'{filename}.json', 'r') as json_file:\n",
    "  lexicons = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir= './data/input/'\n",
    "output_dir= './data/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. encode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "def cosine_similarity(embeddings1, embeddings2):\n",
    "    #Compute cosine-similarits\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "    # print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))\n",
    "    return cosine_scores\n",
    "\n",
    "\n",
    "def vectorize(docs, model_name = 'all-MiniLM-L6-v2'):\n",
    "    # https://www.sbert.net/docs/usage/semantic_textual_similarity.html\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(docs, convert_to_tensor=True)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicon</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>financial_stress</td>\n",
       "      <td>credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>financial_stress</td>\n",
       "      <td>bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>financial_stress</td>\n",
       "      <td>bad credit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>financial_stress</td>\n",
       "      <td>my credit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>financial_stress</td>\n",
       "      <td>loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>mental_health</td>\n",
       "      <td>borderline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>mental_health</td>\n",
       "      <td>histrionic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>mental_health</td>\n",
       "      <td>avoidant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>mental_health</td>\n",
       "      <td>dependent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>mental_health</td>\n",
       "      <td>alogia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>520 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              lexicon         item\n",
       "0    financial_stress  credit card\n",
       "1    financial_stress         bank\n",
       "2    financial_stress   bad credit\n",
       "3    financial_stress    my credit\n",
       "4    financial_stress         loan\n",
       "..                ...          ...\n",
       "515     mental_health   borderline\n",
       "516     mental_health   histrionic\n",
       "517     mental_health     avoidant\n",
       "518     mental_health    dependent\n",
       "519     mental_health       alogia\n",
       "\n",
       "[520 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicons_df = []\n",
    "for lexicon, items in lexicons.items():\n",
    "    for item in items:\n",
    "        lexicons_df.append([lexicon, item])\n",
    "        \n",
    "lexicons_df = pd.DataFrame(lexicons_df, columns = ['lexicon', 'item'])\n",
    "lexicons_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = encoder(lexicons_df.itemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lexicons = stb.\n",
    "df = pd.read_csv("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
